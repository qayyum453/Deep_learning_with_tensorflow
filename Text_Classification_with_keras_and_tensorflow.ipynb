
# coding: utf-8

# In[53]:


#!/usr/bin/env python3.
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import warnings
warnings.filterwarnings("ignore")
from keras.metrics import categorical_accuracy


# In[54]:


import itertools
import os

from keras.preprocessing.sequence import pad_sequences
import numpy as np
import pandas as pd
import tensorflow as tf


from sklearn.preprocessing import LabelBinarizer, LabelEncoder
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, Activation, GlobalMaxPool1D, Dropout, Flatten, Embedding
from keras.layers import Conv1D, MaxPooling1D
from keras.preprocessing import text, sequence
from keras import utils


# This code was tested with TensorFlow v1.13.1
print("You have TensorFlow version", tf.__version__)


# In[55]:


data = pd.read_csv("New_Data4.csv", encoding = 'latin-1')
# printing the head few rows to just see what kind of data do we have
# print(data.head())


# In[56]:


# Confirm that we have a balanced dataset which unfortunately,
# we dont have in this case, but we will still do implementation
# Note: data was randomly shuffled in our BigQuery query
data['tag_value'].value_counts()


# In[57]:



# Split data into train and test
train_size = int(len(data) * .8)
print ("Train size: %d" % train_size)
print ("Test size: %d" % (len(data) - train_size))


# In[58]:


train_posts = data['item title'][:train_size].astype(str)+" "+data['item description'][:train_size].astype(str)
train_tags = data['tag_value'][:train_size]

test_posts = data['item title'][train_size:].astype(str)+" "+data['item description'][train_size:].astype(str)
test_tags = data['tag_value'][train_size:]


# In[59]:


max_words = 1000
tokenize = text.Tokenizer(num_words=max_words)


# In[60]:


tokenize.fit_on_texts(train_posts) # only fit on train
x_train = tokenize.texts_to_sequences(train_posts)
x_test = tokenize.texts_to_sequences(test_posts)

vocab_size = len(tokenize.word_index)+1 # adding 1 because of reserved index


# In[61]:


maxlen = 100
x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)
x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)

# to convert label strings to numbered index
encoder = LabelEncoder()
encoder.fit(train_tags)
y_train = encoder.transform(train_tags)
y_test = encoder.transform(test_tags)


# In[62]:


# # Converts the labels to a one-hot representation
num_classes = np.max(y_train) + 1
y_train = utils.to_categorical(y_train, num_classes)
y_test = utils.to_categorical(y_test, num_classes)


# In[63]:


# Inspect the dimenstions of our training and test data (this is helpful to debug)
print('x_train shape:', x_train.shape)
print('x_test shape:', x_test.shape)
print('y_train shape:', y_train.shape)
print('y_test shape:', y_test.shape)


# In[64]:


# This model trains very quickly and 5 epochs are already more than enough
# Training for more epochs will likely lead to overfitting on this dataset
# You can try tweaking these hyperparamaters when using this model with your own data
batch_size = 32
epochs = 5


# In[65]:


# Build the model

embedding_dim = 50
model = Sequential()

model.add(Embedding(input_dim=vocab_size, 
                           output_dim=embedding_dim, 
                           input_length=maxlen))
model.add(Conv1D(128, 5, activation='relu'))
model.add(GlobalMaxPool1D())
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(Dense(num_classes))
model.add(Activation('softmax'))

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
print(model.summary())


# In[66]:


# model.fit trains the model
# The validation_split param tells Keras what % of our training data should be used in the validation set
# You can see the validation loss decreasing slowly when you run this
# Because val_loss is no longer decreasing we stop training to prevent overfitting
history = model.fit(x_train, y_train,
                    epochs=epochs,
                    verbose=True,
                    validation_split=0.2)


# In[67]:


# Evaluate the accuracy of our trained model
score = model.evaluate(x_test, y_test,
                       batch_size=batch_size, verbose=1)
print('Test score:', score[0])
print('Test accuracy:', score[1])


# In[68]:


# Here's how to generate a prediction on individual examples
text_labels = encoder.classes_

# for i in range(10):
#     prediction = model.predict(np.array([x_test[i]]))
#     predicted_label = text_labels[np.argmax(prediction)]
# #     print(test_posts.iloc[i][:50], "...")
#     print('Actual label:' + test_tags.iloc[i])
#     print("Predicted label: " + predicted_label + "\n")


# In[69]:


# y_softmax = model.predict(x_test)

# y_test_1d = []
# y_pred_1d = []

# for i in range(len(y_test)):
#     probs = y_test[i]
#     index_arr = np.nonzero(probs)
#     one_hot_index = index_arr[0].item(0)
#     y_test_1d.append(one_hot_index)

# for i in range(0, len(y_softmax)):
#     probs = y_softmax[i]
#     predicted_index = np.argmax(probs)
#     y_pred_1d.append(predicted_index)


# In[71]:


test = pd.read_csv('test_dataset.csv', encoding = 'latin-1')
test_data = test['item title'].astype(str)+" "+test['item description'].astype(str)

# tokenize_test = text.Tokenizer(num_words=max_words, char_level=False)
x_test_data = tokenize.texts_to_sequences(test_data)
x_test_data = pad_sequences(x_test_data, padding='post', maxlen=maxlen)
# creating set in which we will store the prediction so that we can than save it to file
predicted_labels = []


# In[72]:


# looping through the whole test dataset to predict its tags
for i in range(len(test_data)):
    test_prediction = model.predict(np.array([x_test_data[i]]))
    predicted_labels.append(text_labels[np.argmax(test_prediction)])
#     print(test_posts.iloc[i][:50], "...")
    print("Predicted label: " + predicted_labels[i] + "\n")
print(list(set(predicted_labels)))


# In[73]:


test['pred_tag_values'] = predicted_labels
# print(test.head())
test.to_csv('Test_with_pred_tag_values_with_tag_count_of_100.csv')


# In[ ]:




